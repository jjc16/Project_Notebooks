{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational_Autoencoder_for_Image_Embedding",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1zl0l5l5utXnk_LHwzrwnabMpxL-FjjuG",
      "authorship_tag": "ABX9TyMo0Ylqc21Yg9LGN8AwBdXV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjc16/Project_Notebooks/blob/master/Variational_Autoencoder_for_Image_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwbtUOa7uf98"
      },
      "source": [
        "# Variational Autoencoder for Image Embedding\n",
        "\n",
        "This notebook demonstrates development of a fast prototype for ranking similar images in an input training set. \n",
        "\n",
        "## Background: \n",
        "Variational autoencoders expand traditional autoencoders by using a trained Linear layer to shape the latent space in a traditional autoencoder to have normally distributed values. It does this by \n",
        "  - training the Linear layer by taking the KL-divergence between the transformed latent space and a $N (0,1)$ distribution (where $N$ is the normal distribution) and \n",
        "  - sampling the reconstructed images from the latent space (assuming a normal distribution) and using the sampled images to calculate the reconstruction loss\n",
        "\n",
        "## Novelties in notebook\n",
        "\n",
        "This notebook also adds a classification head that inputs the latent space and outputs logits for each class in the dataset. The idea here is to not only identify close images (in the latent space) but also return the probable classes.\n",
        "\n",
        "## References:\n",
        "\n",
        "The author utilized several third-party sources to create this notebook:\n",
        "\n",
        "- Data source: http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip \n",
        "\n",
        "- PyTorch Lightning Bolts: https://pytorch-lightning-bolts.readthedocs.io/\n",
        "\n",
        "If you wish to use any code from this notebook in your project, please contact the author: j j c 1 6 [at] y a h o o . c o m. The author makes no warranty, express or implied, about the suitability of this code for any project.\n",
        "\n",
        "We start by installing PyTorch Lighting Bolts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJB6_GDOufX_",
        "outputId": "6afc2f81-4272-48a8-d772-bd573267ccb6"
      },
      "source": [
        "# !pip install torch\n",
        "!pip install lightning-bolts[\"extra\"]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lightning-bolts[extra]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/12/b8c3e5b2407ae151ce1d480434db46fe9f346697c8525ce1b358995d247a/lightning_bolts-0.3.3-py3-none-any.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 25.4MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/b9/59ce5be6679884579c276f5f208587c3312e8323bd7ce27be278b7af98b3/pytorch_lightning-1.3.2-py3-none-any.whl (805kB)\n",
            "\u001b[K     |████████████████████████████████| 808kB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from lightning-bolts[extra]) (1.8.1+cu101)\n",
            "Collecting torchmetrics>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e8/513cd9d0b1c83dc14cd8f788d05cd6a34758d4fd7e4f9e5ecd5d7d599c95/torchmetrics-0.3.2-py3-none-any.whl (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 44.4MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless; extra == \"extra\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/84/72ec52fbac4775c2a5bf0ee5573c922a0cac35eb841907edf56493a5e313/opencv_python_headless-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (38.2MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2MB 73kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from lightning-bolts[extra]) (0.17.3)\n",
            "Requirement already satisfied: matplotlib; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from lightning-bolts[extra]) (3.2.2)\n",
            "Collecting wandb; extra == \"extra\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5f/45439b4767334b868e1c8c35b1b0ba3747d8c21be77b79f09eed7aa3c72b/wandb-0.10.30-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from lightning-bolts[extra]) (7.1.2)\n",
            "Requirement already satisfied: scipy; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from lightning-bolts[extra]) (1.4.1)\n",
            "Requirement already satisfied: torchvision>=0.7; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from lightning-bolts[extra]) (0.9.1+cu101)\n",
            "Collecting scikit-learn>=0.23; extra == \"extra\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.1.1->lightning-bolts[extra]) (4.41.1)\n",
            "Collecting fsspec[http]>=2021.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 50.4MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.1.1->lightning-bolts[extra]) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.1.1->lightning-bolts[extra]) (20.9)\n",
            "Collecting tensorboard!=2.5.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 162kB/s \n",
            "\u001b[?25hCollecting PyYAML<=5.4.1,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->lightning-bolts[extra]) (3.7.4.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2; extra == \"extra\"->lightning-bolts[extra]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2; extra == \"extra\"->lightning-bolts[extra]) (1.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib; extra == \"extra\"->lightning-bolts[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib; extra == \"extra\"->lightning-bolts[extra]) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib; extra == \"extra\"->lightning-bolts[extra]) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib; extra == \"extra\"->lightning-bolts[extra]) (2.4.7)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb; extra == \"extra\"->lightning-bolts[extra]) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 55.4MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb; extra == \"extra\"->lightning-bolts[extra]) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb; extra == \"extra\"->lightning-bolts[extra]) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb; extra == \"extra\"->lightning-bolts[extra]) (3.12.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb; extra == \"extra\"->lightning-bolts[extra]) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb; extra == \"extra\"->lightning-bolts[extra]) (2.3)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23; extra == \"extra\"->lightning-bolts[extra]) (1.0.1)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (1.30.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (1.34.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (0.4.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb; extra == \"extra\"->lightning-bolts[extra]) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb; extra == \"extra\"->lightning-bolts[extra]) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb; extra == \"extra\"->lightning-bolts[extra]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb; extra == \"extra\"->lightning-bolts[extra]) (2.10)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 55.5MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (4.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (1.3.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.1.1->lightning-bolts[extra]) (3.1.0)\n",
            "Building wheels for collected packages: future, subprocess32, pathtools\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=b88642a70f26f8c049a2fe3ba1c9edad01065603d90a99995cbe1a896172183f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=b01f22163c53579bed0b0cd4395bc4fa178c65eff56c1fb64ca36567152c9cfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=67f7f61295f3054e537531978f644f20f078706ab819f1174f7d11ad278a895d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built future subprocess32 pathtools\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: multidict, yarl, async-timeout, aiohttp, fsspec, pyDeprecate, future, torchmetrics, tensorboard, PyYAML, pytorch-lightning, opencv-python-headless, docker-pycreds, subprocess32, smmap, gitdb, GitPython, shortuuid, pathtools, configparser, sentry-sdk, wandb, threadpoolctl, scikit-learn, lightning-bolts\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed GitPython-3.1.17 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 configparser-5.0.2 docker-pycreds-0.4.0 fsspec-2021.5.0 future-0.18.2 gitdb-4.0.7 lightning-bolts-0.3.3 multidict-5.1.0 opencv-python-headless-4.5.2.52 pathtools-0.1.2 pyDeprecate-0.3.0 pytorch-lightning-1.3.2 scikit-learn-0.24.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tensorboard-2.4.1 threadpoolctl-2.1.0 torchmetrics-0.3.2 wandb-0.10.30 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMaCNH1T32tc"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We need to import the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysy4cjwBvAXR"
      },
      "source": [
        "import torch\n",
        "from pl_bolts.models.autoencoders import VAE\n",
        "from matplotlib.pyplot import imshow\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import torchvision\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "\n",
        "device = 'cuda:0'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phHru24V37Vu"
      },
      "source": [
        "## Connect data storage location\n",
        "\n",
        "Now, we connect the data, show a sample image, print the max and min values, and display the shape of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "OVstqgNS254M",
        "outputId": "b7cb7d9c-a9b8-4896-81ef-6b36db8671dd"
      },
      "source": [
        "pth = '/content/drive/MyDrive/geo_sim/geological_similarity/andesite/01LQQ.jpg'\n",
        "with Image.open(pth,'r') as im:\n",
        "  data = np.asarray(im)\n",
        "imshow(data)\n",
        "print(f'Picture Min value: {np.min(data)}')\n",
        "print(f'Picture Max value: {np.max(data)}')\n",
        "print(f'Picture Shape: {data.shape}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Picture Min value: 63\n",
            "Picture Max value: 198\n",
            "Picture Shape: (28, 28, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP4DFYhl4WtJ"
      },
      "source": [
        "## Transforms\n",
        "\n",
        "We will be using a Resnet18 model for the image encoder and decoder. To do this, our images need to be at least size 32 x 32. Also, we need to normalize the images (using values explained in the Resnet18 documentation) and turn the outputs into tensors for model ingestion. The results of doing this are below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmO-34SE7iIK",
        "outputId": "24874194-a705-429d-e64f-2190b564558d"
      },
      "source": [
        "trans_norm = torchvision.transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                         (0.229, 0.224, 0.225))\n",
        "trans_resize = torchvision.transforms.Resize(size=(32,32))\n",
        "trans_ten = torchvision.transforms.ToTensor()\n",
        "# trans_un = torchvision.transforms.Lambda(lambda x: torch.unsqueeze(x,0))\n",
        "trans_comp = torchvision.transforms.Compose([trans_ten, trans_resize, trans_norm])\n",
        "out = trans_comp(data)\n",
        "print(out.shape)\n",
        "print(data.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "(28, 28, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:114: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0iK9mlb4y9C"
      },
      "source": [
        "## Model:\n",
        "\n",
        "Now, we create a variational autoencoder model (VAE). We utilize PyTorch Lighting Bolts to skip writing boilerplate code and greatly accelerate our development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43TxLXeDulQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a816334d-826f-4e63-e014-cffb20dd90ac"
      },
      "source": [
        "model = VAE(input_height=32)\n",
        "print(model)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VAE(\n",
            "  (encoder): ResNetEncoder(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): EncoderBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): EncoderBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): EncoderBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): EncoderBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): EncoderBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): EncoderBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): EncoderBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): EncoderBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (decoder): ResNetDecoder(\n",
            "    (linear): Linear(in_features=256, out_features=8192, bias=True)\n",
            "    (layer1): Sequential(\n",
            "      (0): DecoderBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Sequential(\n",
            "          (0): Interpolate()\n",
            "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (upsample): Sequential(\n",
            "          (0): Sequential(\n",
            "            (0): Interpolate()\n",
            "            (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DecoderBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): DecoderBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Sequential(\n",
            "          (0): Interpolate()\n",
            "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (upsample): Sequential(\n",
            "          (0): Sequential(\n",
            "            (0): Interpolate()\n",
            "            (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DecoderBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): DecoderBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Sequential(\n",
            "          (0): Interpolate()\n",
            "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (upsample): Sequential(\n",
            "          (0): Sequential(\n",
            "            (0): Interpolate()\n",
            "            (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DecoderBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): DecoderBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): DecoderBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (upscale): Interpolate()\n",
            "    (upscale1): Interpolate()\n",
            "    (conv1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  )\n",
            "  (fc_mu): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc_var): Linear(in_features=512, out_features=256, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZNwfah55RBk"
      },
      "source": [
        "## Datasets and Dataloader:\n",
        "\n",
        "We now create PyTorch Datasets and DataLoaders for the training, validation, and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuGNMbDb-Jfl",
        "outputId": "b6fbb5a4-42f0-4294-c4d7-f33b623e3f02"
      },
      "source": [
        "ds = torchvision.datasets.ImageFolder('/content/drive/MyDrive/geo_sim/geological_similarity', transform=trans_comp)\n",
        "from math import floor\n",
        "tr_len = floor(0.8*len(ds))\n",
        "val_ln = floor(0.1*len(ds))\n",
        "tst_ln = len(ds) - tr_len - val_ln\n",
        "train_ds, val_ds, test_ds = torch.utils.data.random_split(ds, [tr_len, val_ln, tst_ln])\n",
        "print(len(train_ds), len(val_ds), len(test_ds))\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=64)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23999 2999 3001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4-DSKT85k86"
      },
      "source": [
        "## Custom VAE Model\n",
        "\n",
        "The VAE model that is created by default doesn't have quite the features that we want. We also want to add a linear classification head to it to predict the classes of the encoded images. To do this, we create a custom model with a an extra linear layer and an overriden step function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVhjrZCZOqfl"
      },
      "source": [
        "class CustomVAE(VAE):\n",
        "  def __init__(self, *args, num_classes=6, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.classification_head = torch.nn.Linear(self.latent_dim, num_classes)\n",
        "\n",
        "  def step(self, batch, batch_idx, alpha=0.1):\n",
        "    loss, logs = super().step(batch, batch_idx)\n",
        "    enc = self.encoder(batch[0])\n",
        "    enc_mu = self.fc_mu(enc)\n",
        "    x_cl = self.classification_head(enc_mu)\n",
        "    labels = batch[1]\n",
        "    # print(labels.shape)\n",
        "    # print(x_cl.shape)\n",
        "    loss_class = alpha*torch.nn.CrossEntropyLoss()(x_cl, labels)\n",
        "    loss = loss + loss_class\n",
        "    # if batch_idx % 10 ==0:\n",
        "    #   print(batch_idx)\n",
        "    return loss, logs\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OROu9lkjJcCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0ef3b6-f698-4908-9e70-5507f352196e"
      },
      "source": [
        "cust_model = CustomVAE(input_height=32)\n",
        "cust_model.load_from_checkpoint('/content/drive/MyDrive/lightning_logs/lightning_logs/version_0/checkpoints/vae_model.ckpt')\n",
        "# params = torch.load('/content/drive/MyDrive/lightning_logs/lightning_logs/version_0/checkpoints/vae_model.ckpt')\n",
        "# model=pl.LightningModule.load_from_checkpoint('/content/drive/MyDrive/lightning_logs/lightning_logs/version_0/checkpoints/vae_model.ckpt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomVAE(\n",
              "  (encoder): ResNetEncoder(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): EncoderBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): EncoderBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): EncoderBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): EncoderBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): EncoderBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (decoder): ResNetDecoder(\n",
              "    (linear): Linear(in_features=256, out_features=8192, bias=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): DecoderBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Interpolate()\n",
              "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (upsample): Sequential(\n",
              "          (0): Sequential(\n",
              "            (0): Interpolate()\n",
              "            (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          )\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): DecoderBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Interpolate()\n",
              "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (upsample): Sequential(\n",
              "          (0): Sequential(\n",
              "            (0): Interpolate()\n",
              "            (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          )\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): DecoderBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Interpolate()\n",
              "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (upsample): Sequential(\n",
              "          (0): Sequential(\n",
              "            (0): Interpolate()\n",
              "            (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          )\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): DecoderBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): DecoderBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (upscale): Interpolate()\n",
              "    (upscale1): Interpolate()\n",
              "    (conv1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  )\n",
              "  (fc_mu): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc_var): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (classification_head): Linear(in_features=256, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceUhdMJW6L7S"
      },
      "source": [
        "## PyTorch Lighting Bolts Trainer\n",
        "\n",
        "Now, we create a trainer and use the .fit method to fit to our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POdMCCh1TGIP"
      },
      "source": [
        "# tr = pl.Trainer(gpus=1,progress_bar_refresh_rate=1, default_root_dir='/content/drive/MyDrive/lightning_logs/')\n",
        "# tr.fit(cust_model, train_dataloader=train_dl, val_dataloaders=val_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh4eqUiHmZzC"
      },
      "source": [
        "## Image embedding\n",
        "\n",
        "Once we are satisfied with the results of the model training, we can use the trained model to embed the images. The process of embedding can take a long time because we have to do several inferences through the model. However, these embeddings can be precomputed and stored in a file for later search.\n",
        "\n",
        "Below, we first get a list of files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YxqZACwbMwE",
        "outputId": "a34c15a2-742a-445f-c5a1-9892dd2ee1b9"
      },
      "source": [
        "import os\n",
        "pth = '/content/drive/MyDrive/geo_sim/geological_similarity/'\n",
        "out = []\n",
        "for path, subdirs, files in os.walk(pth):\n",
        "    for name in files:\n",
        "      if name[0] != '.':\n",
        "        out.append(os.path.join(path, name))\n",
        "print(len(out))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qXq_g6xnWQ4"
      },
      "source": [
        "Next, we embed the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jYcqqe8cY7Y",
        "outputId": "61ec8422-17d8-44cb-c583-e8f64ce54f07"
      },
      "source": [
        "from PIL import Image\n",
        "img_paths = []\n",
        "img_emb_list = []\n",
        "for ii, fl in enumerate(out):\n",
        "  if ii % 100 == 0:\n",
        "    print(f'Iter: {ii}')\n",
        "  img = Image.open(fl)\n",
        "  img = np.asarray(img)\n",
        "  img = cust_model(trans_comp(img).unsqueeze(0))\n",
        "  img_paths.append(fl)\n",
        "  img_emb_list.append(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0\n",
            "Iter: 100\n",
            "Iter: 200\n",
            "Iter: 300\n",
            "Iter: 400\n",
            "Iter: 500\n",
            "Iter: 600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yI6hFASngYZ"
      },
      "source": [
        "Then, we put the embeddings in a dataframe and save the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YtNAPsNgfCO"
      },
      "source": [
        "data_tuples = list(zip(img_paths, img_emb_list))\n",
        "df = pd.DataFrame(data_tuples, columns['Image Path', 'Image Embeding'])\n",
        "df.to_csv('/content/drive/MyDrive/geo_sim/' + 'image_embeddings.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxP2ntXZoAy8"
      },
      "source": [
        "When we're ready to perform the search, we simply load the precomputed embeddings, calculate the distance between each embedding and the embedding of the target image and return the closest images (the minimum distances)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afnkmVxkNV9c"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/geo_sim/' + 'image_embeddings.csv')\n",
        "from scipy.spatial.distance import cosine\n",
        "def search(model, df, img_path, n=5):\n",
        "  ''' Returns a list of nearest images for an input image\n",
        "\n",
        "  Inputs: model - the model to embed the image\n",
        "          df - a dataframe containing images previously embedded using the\n",
        "               embedding model. Should have columns = ['Image Path',\n",
        "               'Image Embedding']\n",
        "          img_path - the path to the test image\n",
        "          n [optional] - the number of nearest images to return\n",
        "\n",
        "  Returns - a dataframe containing information on the nearest images\n",
        "  '''\n",
        "  tmp_df = df.copy()\n",
        "  img = Image.open(fl)\n",
        "  img = np.asarray(img)\n",
        "  emb = cust_model(trans_comp(img).unsqueeze(0))\n",
        "  func = lambda x: cosine(x, emb)\n",
        "  tmp_df = tmp_df.apply(func, 'Image Embedding')\n",
        "  tmp_df = tmp_df.sort(by=['Image Embedding'])\n",
        "\n",
        "  return tmp_df.iloc[0:n,:]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JCgkbGcoUPO"
      },
      "source": [
        "Viola!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdNAYcOuSDCC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}