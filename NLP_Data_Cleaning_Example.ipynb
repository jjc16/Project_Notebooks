{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Data_Cleaning_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17UccIAZcza0XXtGIKev68loQ8yHa8sQa",
      "authorship_tag": "ABX9TyMVN9NPvfJp43rJzZ17na54",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjc16/Project_Notebooks/blob/master/NLP_Data_Cleaning_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTdiH7nBiYbF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EauvbH_23_c2"
      },
      "source": [
        "#NLP on Technology Articles Project\n",
        "\n",
        "## Project Background\n",
        "\n",
        "In this project, a client asked me to identify the \"top 10 keywords\" from a group of articles from a technology magazine. The client gave me freedom to define the problem in a way that made sense.\n",
        "\n",
        "## My definition of \"top 10 keywords\":\n",
        "\n",
        "My definition of the top 10 keywords would be any named entities or named entity-like words in the articles that do not match quirks in the formattting of the articles. For example, I would eliminate a word like \"guest\" because many magazines, podcasts, etc. will introduce their guest or guests at the start of each interview and thus use the word much more frequently than its relative importance to the theme of the article.\n",
        "\n",
        "The results of the exercise are below.\n",
        "\n",
        "## Imports:\n",
        "First, I import and install several necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUCECW_sigsO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import nltk\n",
        "# nltk.download('all')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Mh4rRP4eJa"
      },
      "source": [
        "## Load data:\n",
        "\n",
        "I now load the data into a Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpVooYPBjWGC"
      },
      "source": [
        "pth = '/content/drive/MyDrive/Moodys_Assignment/assignment/News.csv'\n",
        "data = pd.read_csv(pth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uZx0hEi4j-O"
      },
      "source": [
        "## Data Exploration: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "JUVRPT38kf81",
        "outputId": "9dc65872-8ad2-418d-8934-d59e9f47f181"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_id</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>link</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Do chatbots really help you stay productive?</td>\n",
              "      <td>GUEST: When Slack burst onto the workplace sce...</td>\n",
              "      <td>http://venturebeat.com/?p=2141494</td>\n",
              "      <td>2017-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Spanish social advertising company Adsmurai ra...</td>\n",
              "      <td>Barcelona-based social advertising company Ads...</td>\n",
              "      <td>http://venturebeat.com/?p=2141069</td>\n",
              "      <td>2017-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>HTC: No Vive 2 at CES</td>\n",
              "      <td>I\\u2019d wager most people who bought the HTC ...</td>\n",
              "      <td>http://venturebeat.com/?p=2141559</td>\n",
              "      <td>2017-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Chinese firms reportedly ordered to pay Disney...</td>\n",
              "      <td>(Reuters) &amp;#8212;\\xa0A Shanghai court ordered ...</td>\n",
              "      <td>http://venturebeat.com/?p=2141698</td>\n",
              "      <td>2017-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>AWS sees growth in database migrations</td>\n",
              "      <td>Public cloud market leader Amazon Web Services...</td>\n",
              "      <td>http://venturebeat.com/?p=2141375</td>\n",
              "      <td>2017-01-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   news_id  ...        date\n",
              "0        1  ...  2017-01-01\n",
              "1        2  ...  2017-01-01\n",
              "2        3  ...  2017-01-01\n",
              "3        4  ...  2017-01-01\n",
              "4        5  ...  2017-01-01\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxMSJKzL41Ys"
      },
      "source": [
        "We can also specifically print out several examples of the 'content' column, which contains the articles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkjNVNBvkoYB",
        "outputId": "ddff9d77-b3f7-4528-cf41-f4a37b01c507"
      },
      "source": [
        "print(data.content[0] + '\\n')\n",
        "print(data.content[1] + '\\n')\n",
        "print(data.content[2] + '\\n')\n",
        "print(data.content[3] + '\\n')\n",
        "print(data.content[4] + '\\n')\n",
        "print(data.content[5] + '\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GUEST: When Slack burst onto the workplace scene, employees rejoiced. Finally, there was a way to chat with one another without having to send a dreaded email or worse, get up and actually go chat with your coworker face-to-face. Thanks to Slack and a handful of other messaging platforms, businesses could easily communicate across teams using&#160;[&#8230;]\\n\n",
            "\n",
            "Barcelona-based social advertising company Adsmurai has received \\u20ac4 million ($4.2 million) in a second round of funding led by venture capital firm Axon Partners Group, with participation from Banc Sabadell, through its program BStartup10, and Enisa, a Spanish government-funded financing group. Launched in 2014 by Marc Elena, Otto W\\xfcst and Juan Antonio Robles, Adsmurai specializes&#160;[&#8230;]\\n\n",
            "\n",
            "I\\u2019d wager most people who bought the HTC Vive love the unit but wish a new version would bring key improvements. A slimmer design and lighter cord, a better fit\\xa0for\\xa0the face and more ergonomic controllers without hard-to-reach grip buttons are all near the top of the early adopter wish list. HTC relies on Valve\\u2019s lighthouse&#160;[&#8230;]\\n\n",
            "\n",
            "(Reuters) &#8212;\\xa0A Shanghai court ordered two Chinese firms to pay Walt Disney Co and Pixar more than 1.35 million yuan ($194,440) compensation for copying parts of their hit movies &#8220;Cars&#8221; and &#8220;Cars 2&#8221;, the official Xinhua news agency reported on Saturday. The ruling is the latest in a slew of intellectual property wins for large&#160;[&#8230;]\\n\n",
            "\n",
            "Public cloud market leader Amazon Web Services (AWS) offers a\\xa0lot of services &#8212; 83 by my count. Some of them are more popular than others. Without question, the core EC2 computing service and S3 storage service are among the most popular. But then what? AWS&#8217; parent company, Amazon.com, doesn&#8217;t break out sales numbers for all&#160;[&#8230;]\\n\n",
            "\n",
            "OPINION: I can&#8217;t do this anymore. I don&#8217;t enjoy staring blankly into this\\xa0abyss every night, my clenched jaw\\xa0lit by the glow of someone else&#8217;s stream of consciousness (not\\xa0even in chronological order). I don&#8217;t want to talk\\xa0about editing tweets, or argue with\\xa0Twitter eggs, or\\xa0watch\\xa0Trump fuss\\xa0about Vanity Fair. Scanning my\\xa0feed like an addict doesn&#8217;t help me do my&#160;[&#8230;]\\n\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doSveh9m5Ab1"
      },
      "source": [
        "As can be seen, the text in it's native format is messy, including several artifacts from the data accumulation process and punctuation. Also, there are several common English words in the data that are less helpful in determining keywords from the article that need to be removed. I will write and use several helper functions to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEVvJW8alZ0T"
      },
      "source": [
        "#Clean bad words\n",
        "bad_words = ['&', '#','\\\\']\n",
        "def clean_bad_words(lst, bad_words = ['&', '#','\\\\']):\n",
        "  out = []\n",
        "  for word in lst:\n",
        "    spl = [word[ii] for ii in range(len(word))]\n",
        "    # print(spl)\n",
        "    tst = [s in bad_words for s in spl]\n",
        "    # print(tst)\n",
        "    if not any(tst):\n",
        "      # print(t)\n",
        "      out.append(word)\n",
        "  return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asCzRbRloFIm"
      },
      "source": [
        "#strip punctional and symbols\n",
        "def strip_symbols(lst, symbols='[()$.:,;]'):\n",
        "  return [l.strip(symbols) for l in lst]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW_JxAxlpu7K"
      },
      "source": [
        "#generate list of common words\n",
        "my_file = open(\"/content/drive/MyDrive/Moodys_Assignment/assignment/common_words.txt\", \"r\")\n",
        "content = my_file.read()\n",
        "common_words = content.split(\"\\n\")\n",
        "my_file.close()\n",
        "# print(common_words)\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "common_words += stop_words"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxTS_a-4pDoZ"
      },
      "source": [
        "#remove common words\n",
        "def remove_common_words(lst, common_words):\n",
        "  return [l for l in lst if l not in common_words]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJmriFnD6V2N"
      },
      "source": [
        "## Keyword extraction:\n",
        "Finally, I am ready to extract the key words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqNsnFRhl-q1"
      },
      "source": [
        "def extract_key_words(data, bad_words, common_words):\n",
        "  out_list = []\n",
        "  for ii in range(len(data)):\n",
        "    lst = data['content'][ii].lower().split(' ')\n",
        "    out = clean_bad_words(lst, bad_words)\n",
        "    out = strip_symbols(out)\n",
        "    out = remove_common_words(out, common_words)\n",
        "    out_list += out\n",
        "  return out_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5iVVHmdmEit",
        "outputId": "efebf972-39d6-4435-87a3-68eb7748e50f"
      },
      "source": [
        "out_list = extract_key_words(data, bad_words, common_words)\n",
        "print(out_list[0:200])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['guest', 'slack', 'burst', 'onto', 'workplace', 'scene', 'employees', 'rejoiced', 'finally', 'chat', 'another', 'without', 'dreaded', 'email', 'worse', 'actually', 'chat', 'coworker', 'face-to-face', 'thanks', 'slack', 'handful', 'messaging', 'platforms', 'businesses', 'easily', 'communicate', 'across', 'teams', 'barcelona-based', 'social', 'advertising', 'adsmurai', 'received', '4.2', 'funding', 'venture', 'firm', 'axon', 'partners', 'participation', 'banc', 'sabadell', 'program', 'bstartup10', 'enisa', 'spanish', 'government-funded', 'financing', 'launched', '2014', 'marc', 'elena', 'otto', 'juan', 'antonio', 'robles', 'adsmurai', 'wager', 'htc', 'vive', 'version', 'improvements', 'slimmer', 'lighter', 'cord', 'ergonomic', 'controllers', 'without', 'hard-to-reach', 'grip', 'buttons', 'adopter', 'htc', 'relies', 'reuters', 'shanghai', 'court', 'ordered', 'chinese', 'firms', 'walt', 'disney', 'co', 'pixar', '1.35', 'yuan', '194,440', 'compensation', 'copying', 'parts', 'movies', 'official', 'xinhua', 'news', 'agency', 'reported', 'saturday', 'ruling', 'latest', 'slew', 'intellectual', 'wins', 'public', 'leader', 'amazon', 'web', 'services', 'aws', 'offers', 'services', '83', 'popular', 'others', 'without', 'core', 'ec2', 'computing', 'service', 's3', 'storage', 'service', 'popular', 'what?', 'amazon.com', 'sales', 'numbers', 'opinion', 'anymore', 'enjoy', 'staring', 'blankly', 'clenched', 'glow', 'someone', 'consciousness', 'chronological', 'editing', 'tweets', 'argue', 'eggs', 'vanity', 'scanning', 'addict', 'google', 'assistant', 'smart', 'speaker', 'google', 'opinions', '2017', 'resolution', 'ai', 'assistant', 'things', 'novel', 'calligraphy', 'answers', 'review', '2016', 'macbook', 'pro', 'scandalous', 'relative', 'predecessor', 'missing', 'ports', 'battery', 'skimpy', 'keyboard', 'department', 'expensive', 'following', 'reports', 'battery', 'issues', 'ultimately', 'chose', 'estimate', 'battery', 'consumer', 'reports', 'declined', 'recommend', 'device', 'personally?', 'really', 'artificial', 'intelligence', 'ai', 'mainstream', 'google', 'executive', 'sundar', 'pichai', 'bite', 'saying', 'going', 'squished', 'ai']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI_rQDf39Oc1"
      },
      "source": [
        "## Count Frequencies:\n",
        "\n",
        "I can now use list and a Python tool to count the relative frequencies of the words in the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa38D24lonMs",
        "outputId": "7c5fe1c0-f903-43ce-8e66-b604ec9e0238"
      },
      "source": [
        "counter = Counter(out_list).most_common(50)\n",
        "print(counter)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('', 10340), ('today', 2774), ('announced', 2042), ('games', 1622), ('guest', 1224), ('google', 1115), ('mobile', 1053), ('technology', 889), ('release', 814), ('app', 760), ('companies', 754), ('data', 736), ('service', 735), ('reality', 730), ('platform', 724), ('tech', 695), ('ai', 685), ('microsoft', 673), ('startup', 672), ('vr', 657), ('video', 653), ('pc', 646), ('years', 632), ('billion', 618), ('u.s', 613), ('virtual', 610), ('2017', 602), ('launched', 601), ('online', 601), ('software', 592), ('developers', 574), ('reuters', 569), ('users', 567), ('facebook', 561), ('series', 543), ('funding', 542), ('according', 541), ('percent', 538), ('intelligence', 535), ('gaming', 530), ('available', 510), ('launch', 503), ('business', 491), ('10', 489), ('latest', 477), ('artificial', 476), ('digital', 473), ('developer', 473), ('amazon', 472), ('xbox', 458)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQm2JCIUzICT"
      },
      "source": [
        "# Final Answers:\n",
        "\n",
        "For the final answer, we have to go through by hand and use human judgement to return the most common words. The constraints are to remove anything that can't be a named entity (like 'today', 'announced', and 'guest')\n",
        "\n",
        "**Answers:**\n",
        "\n",
        "1. games\n",
        "2. Google\n",
        "3. mobile\n",
        "4. technology\n",
        "5. release\n",
        "6. app\n",
        "7. data\n",
        "8. service\n",
        "9. reality (i.e. <em>virtual reality</em>)\n",
        "10. platform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktdG02Vs3x_E"
      },
      "source": [
        "# Extensions:\n",
        "\n",
        "If I had several weeks to do the project, this is how I would extend it:\n",
        "\n",
        "### Data Preparation:\n",
        "  - Improve the data cleaning and munging functions to further remove common words that can't possibly be named entities or named entities-like\n",
        "  - Explore word embedding models to clarify cases when named entities might be overcounted because of their similarities to common words (e.g. cloud in cloud computing)\n",
        "  - Check terms for aliases (e.g. AWS = Amazon cloud computing)\n",
        "  - Improve removal of problem words and symbols so that keywords aren't potentially lost\n",
        "\n",
        "### Data Comprehension:\n",
        "  - Connections between themes (as measured by things like cooccurences and correlations between words being present in articles)\n",
        "  - Temporal effects on data (when terms trend and decline)\n",
        "  - Intra and inter article theme extraction\n",
        "\n",
        "### Pipeline:\n",
        "  - Build a pipeline to automate similar extractions across classes of articles\n",
        "  - Deploy the pipeline to the cloud server (GCP, AWS) with REST API and/or interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxRq9gTD3sW4"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}